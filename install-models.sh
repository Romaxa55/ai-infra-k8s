#!/bin/bash

# –°–∫—Ä–∏–ø—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö AI –º–æ–¥–µ–ª–µ–π
# –î–ª—è —Å–µ—Ä–≤–µ—Ä–∞: 254GB RAM + 70 cores + RTX 3050

set -e

echo "üöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¢–û–ü AI –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–æ—â–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞"
echo "üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: 254GB RAM + 70 cores + RTX 3050 (6GB)"

# –ü–æ–ª—É—á–∞–µ–º –∏–º—è –ø–æ–¥–∞ Ollama
OLLAMA_POD=$(kubectl get pods -n ai-infra -l app.kubernetes.io/name=ollama -o jsonpath='{.items[0].metadata.name}')
if [ -z "$OLLAMA_POD" ]; then
    echo "‚ùå Ollama pod –Ω–µ –Ω–∞–π–¥–µ–Ω!"
    exit 1
fi

echo "üì¶ –ù–∞–π–¥–µ–Ω Ollama pod: $OLLAMA_POD"

# –§—É–Ω–∫—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏
install_model() {
    local model=$1
    local description=$2
    echo "‚¨áÔ∏è  –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º $model - $description"
    kubectl exec -it $OLLAMA_POD -n ai-infra -- ollama pull $model
    echo "‚úÖ $model —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"
}

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ–µ –º–µ—Å—Ç–æ
echo "üíæ –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ–µ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ..."
kubectl exec -it $OLLAMA_POD -n ai-infra -- df -h /root/.ollama/

read -p "ü§î –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫—É? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "‚ùå –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞"
    exit 1
fi

echo "üéØ –ù–∞—á–∏–Ω–∞–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É –º–æ–¥–µ–ª–µ–π..."

# –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ë—ã—Å—Ç—Ä—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è GPU (—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–º–∏)
echo "üèÉ‚Äç‚ôÇÔ∏è –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±—ã—Å—Ç—Ä—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è GPU..."
install_model "llama3.1:8b" "–ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è GPU (5GB)"
install_model "mistral:7b" "–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (4GB)"
install_model "qwen2.5-coder:14b" "–ë—ã—Å—Ç—Ä—ã–π –∫–æ–¥–∏–Ω–≥ –¥–ª—è GPU (8GB)"

# –ü–†–ò–û–†–ò–¢–ï–¢ 2: –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
echo "üë®‚Äçüíª –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏..."
install_model "devstral:24b" "–õ—É—á—à–∞—è –¥–ª—è coding agents (14GB)"
install_model "qwen2.5-coder:32b" "–¢–û–ü –∫–æ–¥–∏–Ω–≥ –º–æ–¥–µ–ª—å (19GB)"
install_model "deepseek-r1:32b" "Reasoning –º–æ–¥–µ–ª—å (19GB)"

# –ü–†–ò–û–†–ò–¢–ï–¢ 3: –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ (—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤ —Ñ–æ–Ω–µ)
echo "üêò –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ (–≤ —Ñ–æ–Ω–µ)..."

echo "‚¨áÔ∏è  –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—É—é —É—Å—Ç–∞–Ω–æ–≤–∫—É –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π..."
{
    echo "üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º llama3.3:70b (40GB)..."
    kubectl exec -i $OLLAMA_POD -n ai-infra -- ollama pull llama3.3:70b
    echo "‚úÖ llama3.3:70b —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"
} &

{
    echo "üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º deepseek-r1:70b (40GB)..."  
    kubectl exec -i $OLLAMA_POD -n ai-infra -- ollama pull deepseek-r1:70b
    echo "‚úÖ deepseek-r1:70b —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"
} &

{
    echo "üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º codellama:70b (40GB)..."
    kubectl exec -i $OLLAMA_POD -n ai-infra -- ollama pull codellama:70b  
    echo "‚úÖ codellama:70b —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"
} &

echo "‚è∞ –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –≤ —Ñ–æ–Ω–µ..."
echo "üìã –ú–æ–∂–µ—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å: kubectl logs -f $OLLAMA_POD -n ai-infra"

# –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ì–ò–ì–ê–ù–¢–°–ö–£–Æ –º–æ–¥–µ–ª—å –æ—Ç–¥–µ–ª—å–Ω–æ
read -p "üí™ –•–æ—Ç–∏—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ì–ò–ì–ê–ù–¢–°–ö–£–Æ –º–æ–¥–µ–ª—å deepseek-v3:671b (~400GB)? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "ü¶£ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ì–ò–ì–ê–ù–¢–°–ö–£–Æ –º–æ–¥–µ–ª—å deepseek-v3:671b..."
    echo "‚ö†Ô∏è  –≠—Ç–æ –∑–∞–π–º–µ—Ç –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –º–µ—Å—Ç–∞ (~400GB)"
    kubectl exec -it $OLLAMA_POD -n ai-infra -- ollama pull deepseek-v3:671b
    echo "‚úÖ deepseek-v3:671b —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!"
fi

echo "‚è≥ –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ñ–æ–Ω–æ–≤—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–æ–∫..."
wait

echo ""
echo "üéâ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!"
echo "üìã –ü—Ä–æ–≤–µ—Ä–∏–º —Å–ø–∏—Å–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:"
kubectl exec -it $OLLAMA_POD -n ai-infra -- ollama list

echo ""
echo "üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é:"
echo "  ‚Ä¢ llama3.1:8b - –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ (–ø–æ–ª–Ω–æ—Å—Ç—å—é –≤ GPU)"
echo "  ‚Ä¢ devstral:24b - –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –∏ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤"
echo "  ‚Ä¢ qwen2.5-coder:32b - –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –∫–æ–¥–∏–Ω–≥–∞"
echo "  ‚Ä¢ llama3.3:70b - –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å—é RAM)"
echo "  ‚Ä¢ deepseek-r1:70b - –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ reasoning"

echo ""
echo "üåê –û—Ç–∫—Ä–æ–π—Ç–µ Open WebUI: http://ai.local"
echo "‚ú® –ù–∞—Å–ª–∞–∂–¥–∞–π—Ç–µ—Å—å –º–æ—â—å—é AI –Ω–∞ –≤–∞—à–µ–º —Å–µ—Ä–≤–µ—Ä–µ!" 