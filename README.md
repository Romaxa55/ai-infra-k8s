# AI Infrastructure –Ω–∞ Kubernetes

–ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è AI-–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–∞ Kubernetes —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è LLM –∏ ML —Å–µ—Ä–≤–∏—Å–æ–≤.

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
- **Ollama** - –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ LLM –º–æ–¥–µ–ª–µ–π —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
- **Open WebUI** - –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM (–∞–Ω–∞–ª–æ–≥ ChatGPT)
- **Jupyter** - Notebooks –¥–ª—è ML/AI —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Å GPU
- **MLflow** - Tracking —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ ML (–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è)
- **MinIO** - S3-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (–ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è)

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- Kubernetes –∫–ª–∞—Å—Ç–µ—Ä (K3s/K8s)
- –ù–æ–¥–∞ —Å NVIDIA GPU
- NVIDIA drivers —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
- Longhorn –¥–ª—è storage

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
```bash
git clone <repo-url>
cd ai-infra-k8s
```

### 2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ
```bash
chmod +x deploy.sh
./deploy.sh
```

### 3. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –¥–æ—Å—Ç—É–ø
–î–æ–±–∞–≤—å—Ç–µ –≤ `/etc/hosts`:
```
<IP_–í–ê–®–ï–ì–û_–ö–õ–ê–°–¢–ï–†–ê> ai.local jupyter.local
```

### 4. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª–∏ –≤ Ollama
```bash
# –õ–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞—á–∞–ª–∞ (3B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
kubectl exec -it deployment/ollama -n ollama -- ollama pull llama3.2:3b

# –ú–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∞
kubectl exec -it deployment/ollama -n ollama -- ollama pull codellama:7b

# –ë–æ–ª–µ–µ –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ —Ö–≤–∞—Ç–∞–µ—Ç VRAM)
kubectl exec -it deployment/ollama -n ollama -- ollama pull llama3.2:8b
```

## üîó –î–æ—Å—Ç—É–ø –∫ —Å–µ—Ä–≤–∏—Å–∞–º

- **Open WebUI**: http://ai.local
- **Jupyter**: http://jupyter.local (—Ç–æ–∫–µ–Ω: `ai-jupyter-token`)

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

```bash
# –°—Ç–∞—Ç—É—Å –ø–æ–¥–æ–≤
kubectl get pods -n ollama
kubectl get pods -n jupyter

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
kubectl describe node k3s-worker1 | grep nvidia.com/gpu

# –õ–æ–≥–∏ Ollama
kubectl logs -f deployment/ollama -n ollama
```

## üéÆ GPU –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

–í–∞—à–∞ –Ω–æ–¥–∞ —Å GPU:
- **GPU**: NVIDIA GeForce RTX 3050 (6GB VRAM)
- **CPU**: 40 cores
- **Memory**: ~147GB RAM
- **CUDA**: 12.2

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è RTX 3050:
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª–∏ –¥–æ 7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –î–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é
- –ù–∞—Å—Ç—Ä–æ–π—Ç–µ GPU sharing –º–µ–∂–¥—É —Å–µ—Ä–≤–∏—Å–∞–º–∏

## üìù –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã

### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏ Ollama:
```bash
# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
kubectl exec -it deployment/ollama -n ollama -- ollama list

# –£–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
kubectl exec -it deployment/ollama -n ollama -- ollama rm <model-name>

# –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏
kubectl exec -it deployment/ollama -n ollama -- ollama run llama3.2:3b
```

### –û—Ç–ª–∞–¥–∫–∞:
```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
kubectl describe node k3s-worker1 | grep -A 10 -B 10 nvidia

# –õ–æ–≥–∏ –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
kubectl logs -f deployment/ollama -n ollama
kubectl logs -f deployment/open-webui -n ollama
kubectl logs -f deployment/jupyter -n jupyter
```

## üîß –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è

### –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤:
–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö:
- `ollama/ollama-deployment.yaml` - —Ä–µ—Å—É—Ä—Å—ã Ollama
- `jupyter/jupyter-deployment.yaml` - —Ä–µ—Å—É—Ä—Å—ã Jupyter
- `open-webui/open-webui-deployment.yaml` - —Ä–µ—Å—É—Ä—Å—ã WebUI

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤:
1. –°–æ–∑–¥–∞–π—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞
2. –î–æ–±–∞–≤—å—Ç–µ deployment.yaml
3. –û–±–Ω–æ–≤–∏—Ç–µ deploy.sh

## üõ†Ô∏è Troubleshooting

### GPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω:
```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ NVIDIA device plugin
kubectl get pods -n kube-system | grep nvidia

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ labels –Ω–æ–¥—ã
kubectl get nodes --show-labels | grep gpu
```

### –ü—Ä–æ–±–ª–µ–º—ã —Å storage:
```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Longhorn
kubectl get pods -n longhorn-system

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ PVC
kubectl get pvc -A
```

## üìà –ü–ª–∞–Ω—ã —Ä–∞–∑–≤–∏—Ç–∏—è

- [ ] MLflow –¥–ª—è tracking —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
- [ ] MinIO –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–µ–π
- [ ] Grafana –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ GPU
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
- [ ] Model serving —Å TensorFlow Serving
- [ ] Vector database (Qdrant/Weaviate)

## ü§ù –í–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç

1. Fork —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
2. –°–æ–∑–¥–∞–π—Ç–µ feature branch
3. Commit –∏–∑–º–µ–Ω–µ–Ω–∏—è
4. Push –≤ branch
5. –°–æ–∑–¥–∞–π—Ç–µ Pull Request

## üìÑ –õ–∏—Ü–µ–Ω–∑–∏—è

MIT License
